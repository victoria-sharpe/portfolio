{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victoria-sharpe/portfolio/blob/main/GPT_NeoX_Probabilities_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM4I7wTuhPoF"
      },
      "source": [
        "## Description\n",
        "This script computes word-level log probabilities, based on token probabilities extracted from the LLM GPT-NeoX, for any given text.\n",
        "\n",
        "Probabilities can be computed based on the full context, or based on a range of context lengths lengths. For example, for the text \"I went to the store last Thursday\", we could compute the probability of \"Thursday\" given a context length of 2--P(\"Thursday\" | \"store last\")--or a context length of 3--P(\"Thursday\" | \"the store last\")--and so on.\n",
        "\n",
        "For theory and use cases, see:\n",
        "<br>\n",
        "Sharpe, V., Mackinley, M., Eddine, S. N., Wang, L., Palaniyappan, L., & Kuperberg, G. R. (2025). Selective insensitivity to global vs. local linguistic context in speech produced by patients with untreated psychosis and positive thought disorder. Biological Psychiatry.\n",
        "\n",
        "For example output, see .\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2nA9_Q9ChH8D"
      },
      "outputs": [],
      "source": [
        "## Install packages if needed\n",
        "\n",
        "%%capture\n",
        "\n",
        "!pip install transformers\n",
        "!pip install minicons\n",
        "!pip install pandas\n",
        "!pip install numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "maXh43APhiuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "117bc19e-b90a-4fbb-cc06-2c7d43ac6095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "## Load necessary packages\n",
        "\n",
        "%%capture\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from minicons import scorer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ACEH0FLtiLdn"
      },
      "outputs": [],
      "source": [
        "\n",
        "%%capture\n",
        "\n",
        "## Load the LLM\n",
        "model = scorer.IncrementalLMScorer(\"EleutherAI/gpt-neo-1.3B\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "iiocSsoPih38"
      },
      "outputs": [],
      "source": [
        "## Create a function to convert extracted token probabilities to word-level probabilities (known as \"cloze\" probabilities)\n",
        "## We do this using the rule of joint probabilities: P(A & B) = P(A) + P(B | A)\n",
        "\n",
        "def get_cloze(text, log_prob_data):\n",
        "\n",
        "    \"\"\"Combines token-level log probabilities into word-level probabilities.\n",
        "\n",
        "    Parameters:\n",
        "      text (str): text for which token probability values were obtained\n",
        "      log_prob_data: output of model.token_score(text)\n",
        "\n",
        "    Returns:\n",
        "      pandas dataframe with word-level probabilities\n",
        "\n",
        "   \"\"\"\n",
        "    text_list = text.split() #split text into list of words\n",
        "    text_list = [text_list[0]] + [' ' + x for x in text_list[1:len(text_list)]]\n",
        "    probs = [] #create empty list to begin tracking word-level probabilities\n",
        "    word_count = 0 #track number of words accounted for\n",
        "\n",
        "    token_prob = 0 #dummy probability to start us out\n",
        "    placeholder = \"\" #dummy text\n",
        "\n",
        "    #parse nested arrays of tokens and probabilities\n",
        "    token_list =[inner_tuple[0] for inner_list in log_prob_data for inner_tuple in inner_list]\n",
        "    prob_list = [inner_tuple[1] for inner_list in log_prob_data for inner_tuple in inner_list]\n",
        "\n",
        "    #loop through tokens, matching to words and combining probabilities as we go\n",
        "    for prob, token in zip(prob_list, token_list):\n",
        "        placeholder += token.strip() #add the current token to placeholder text\n",
        "\n",
        "        #if placeholder text makes a full word from text, add word prob to list\n",
        "        #then, reset to start a new word\n",
        "        if placeholder.replace('Ä ', ' ') == text_list[word_count]:\n",
        "          if pd.isna(prob):\n",
        "            probs.append(prob) #special case: if first token, prob = NA\n",
        "\n",
        "          else:\n",
        "            probs.append(prob + token_prob) #append combined prob to list\n",
        "\n",
        "          placeholder = \"\"\n",
        "          token_prob = 0\n",
        "          word_count += 1\n",
        "\n",
        "        #otherwise, add the current log prob to token_prob and keep going\n",
        "        else:\n",
        "          if pd.isna(prob):\n",
        "            token_prob = 1 #special case: if first token, prob = NA\n",
        "\n",
        "          else:\n",
        "            token_prob += prob #add current log prob to previous\n",
        "\n",
        "\n",
        "    probs[0] = np.nan  #set prob of first word (which has no context) to NaN\n",
        "    #organize word-level probabilities into a dataframe\n",
        "    return probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "5xryn3kAPsFZ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "def main(text, context_range = (1,50), compute_full_context = True):\n",
        "    \"\"\"Computes probabilities for each word in a given text, based on a range of context lengths and/or full context.\n",
        "    For efficiency, assigns probabilities on the diagonal for a  \"moving window\" of text\n",
        "    E.g., for a moving window of size M, we get probability of word M given context length M-1,\n",
        "    probability of word M-1 given context length M-2, probability of word M-2 given\n",
        "    context length M-3, and so on.\n",
        "\n",
        "    Parameters:\n",
        "      text (str): string to obtain probabilities for\n",
        "      context_range (tuple): of form (min_context, max_context),\n",
        "        where min_context is the minimum context length to compute probabilities for &\n",
        "        max_context is the maximum context length to compute probabilities for.\n",
        "        If None, do not compute probabilities for a range of context lengths.\n",
        "      compute_full_context (bool): if True, compute probabilities for full context, regardless of length\n",
        "\n",
        "    Returns:\n",
        "      pandas dataframe with word-level probabilities for each word in text, for each requested context length\n",
        "\n",
        "   \"\"\"\n",
        "  #set up dataframe with one word per row\n",
        "    text_list = text.split()\n",
        "    df = pd.DataFrame({'word': text_list, 'word_idx': range(0, len(text_list))})\n",
        "\n",
        "  #compute probabilities based on full context\n",
        "    if compute_full_context:\n",
        "      token_logProbs  = model.token_score(text)\n",
        "      df['cloze_fullContext'] = get_cloze(text, token_logProbs)\n",
        "\n",
        "\n",
        "  #compute probabilities based on range of context lengths\n",
        "    if context_range is not None:\n",
        "      min_context = context_range[0]\n",
        "      max_context = context_range[1]\n",
        "\n",
        "    #set up columns - one for each context length in range\n",
        "\n",
        "\n",
        "      start = 0 #our context window begins at the first word\n",
        "      if len(text_list) >= max_context + 1:\n",
        "        end = max_context #if the utterance is long enough, window length is max_context + 1\n",
        "      else:\n",
        "        end = len(text_list) - 1  #otherwise window ends at the end of the utterance\n",
        "\n",
        "      while start <= end - min_context:\n",
        "        sub_text = text_list[start:end+1]\n",
        "        orig_idx = [i for i in range(start, end+1)]\n",
        "\n",
        "        # Compute the token-level log probabilities\n",
        "        token_logProbs  = model.token_score(' '.join(sub_text))\n",
        "\n",
        "        # Transform the token-level log probabilities into word-level probabilities\n",
        "        word_logProbs = get_cloze(' '.join(sub_text), token_logProbs)\n",
        "\n",
        "        # Assign probabilities on the diagonal to their correct location within the df\n",
        "        for orig_i, new_i in zip(orig_idx[1:], range(1,len(word_logProbs))):\n",
        "          colname = \"cloze_\" + str(new_i)+\"WordContext\"\n",
        "          df.loc[orig_i, colname] = word_logProbs[new_i]\n",
        "\n",
        "        start += 1\n",
        "\n",
        "        if end < len(text_list):\n",
        "          end += 1\n",
        "\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"And as I sat there brooding on the old, unknown world, I thought of Gatsby's wonder when he first picked out the green light at the end of Daisy's dock.\"\n",
        "\n",
        "d = main(text, context_range = (1,10), compute_full_context = True)\n",
        "print(d)\n",
        "d.to_csv('GPTNeoX_Pipeline_Example_Output.csv')"
      ],
      "metadata": {
        "id": "5o_XSqY4OUnc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddc0197f-d427-40e1-f2c3-0f86385c6831"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        word  word_idx  cloze_fullContext  cloze_1WordContext  \\\n",
            "0        And         0                NaN                 NaN   \n",
            "1         as         1          -5.448388           -5.448388   \n",
            "2          I         2          -2.447530           -6.273293   \n",
            "3        sat         3          -3.940406           -8.431185   \n",
            "4      there         4          -0.891540           -8.010176   \n",
            "5   brooding         5          -7.087817          -17.254023   \n",
            "6         on         6          -2.833630           -3.709713   \n",
            "7        the         7          -1.092057           -5.068345   \n",
            "8       old,         8         -10.124781          -11.098182   \n",
            "9    unknown         9          -7.632751           -9.746181   \n",
            "10    world,        10          -4.988406          -10.587034   \n",
            "11         I        11          -1.442773           -4.400987   \n",
            "12   thought        12          -2.347779           -5.856423   \n",
            "13        of        13          -0.912090           -3.284716   \n",
            "14  Gatsby's        14         -13.014043          -18.116781   \n",
            "15    wonder        15          -9.067266          -11.680614   \n",
            "16      when        16          -5.824090           -7.282102   \n",
            "17        he        17          -0.421640           -3.416118   \n",
            "18     first        18          -2.242060           -6.948030   \n",
            "19    picked        19          -6.840596           -9.721390   \n",
            "20       out        20          -3.027179           -7.787097   \n",
            "21       the        21          -1.101727           -5.577607   \n",
            "22     green        22          -5.566880           -9.436159   \n",
            "23     light        23          -3.299476           -7.189514   \n",
            "24        at        24          -2.985293           -6.686411   \n",
            "25       the        25          -0.399700           -5.349692   \n",
            "26       end        26          -0.955904           -6.224792   \n",
            "27        of        27          -0.008302           -6.581636   \n",
            "28   Daisy's        28          -1.828954          -18.053696   \n",
            "29     dock.        29          -2.739823          -14.242465   \n",
            "\n",
            "    cloze_2WordContext  cloze_3WordContext  cloze_4WordContext  \\\n",
            "0                  NaN                 NaN                 NaN   \n",
            "1                  NaN                 NaN                 NaN   \n",
            "2            -2.447530                 NaN                 NaN   \n",
            "3            -7.879186           -3.940406                 NaN   \n",
            "4            -3.251724           -2.364179           -0.891540   \n",
            "5            -7.219316           -7.278152           -7.593451   \n",
            "6            -3.115019           -3.037168           -2.745222   \n",
            "7            -1.141071           -0.865623           -0.985020   \n",
            "8           -10.770145          -11.243055           -9.037256   \n",
            "9            -8.236490           -8.573506           -8.349156   \n",
            "10          -10.282799           -5.558563           -6.108421   \n",
            "11           -3.847799           -4.712970           -4.706772   \n",
            "12           -4.186718           -3.955794           -3.955130   \n",
            "13           -3.645208           -3.341144           -3.018072   \n",
            "14          -14.757601          -13.907835          -13.476630   \n",
            "15          -11.009162           -9.832219           -9.495943   \n",
            "16           -8.239998           -6.380717           -4.563552   \n",
            "17           -2.539588           -1.284944           -0.817830   \n",
            "18           -4.832182           -4.132527           -2.171854   \n",
            "19           -8.995135           -6.831827           -6.170875   \n",
            "20           -3.879781           -4.199801           -3.741904   \n",
            "21           -2.102386           -2.152703           -1.815860   \n",
            "22           -8.037814           -7.078388           -6.958533   \n",
            "23           -3.121699           -3.114854           -3.945864   \n",
            "24           -4.506287           -4.940407           -5.001468   \n",
            "25           -1.208681           -0.668911           -0.613892   \n",
            "26           -2.354516           -2.591758           -0.883831   \n",
            "27           -0.446189           -0.221170           -0.281822   \n",
            "28          -14.601877          -13.387161          -13.763023   \n",
            "29          -12.675487          -11.209594          -12.287062   \n",
            "\n",
            "    cloze_5WordContext  cloze_6WordContext  cloze_7WordContext  \\\n",
            "0                  NaN                 NaN                 NaN   \n",
            "1                  NaN                 NaN                 NaN   \n",
            "2                  NaN                 NaN                 NaN   \n",
            "3                  NaN                 NaN                 NaN   \n",
            "4                  NaN                 NaN                 NaN   \n",
            "5            -7.087817                 NaN                 NaN   \n",
            "6            -2.564408           -2.833630                 NaN   \n",
            "7            -1.038908           -1.027489           -1.092057   \n",
            "8            -9.514754          -10.400877          -10.138350   \n",
            "9            -7.308214           -7.337660           -8.207990   \n",
            "10           -6.875520           -4.775599           -5.122115   \n",
            "11           -4.333910           -4.061244           -4.740443   \n",
            "12           -3.882290           -4.024366           -3.733785   \n",
            "13           -2.995137           -2.387519           -2.458038   \n",
            "14          -13.762627          -13.851200          -13.676113   \n",
            "15           -9.282410           -9.078667           -9.359328   \n",
            "16           -5.349020           -5.090937           -5.487342   \n",
            "17           -0.662247           -1.773547           -0.571826   \n",
            "18           -2.053203           -2.072091           -2.971220   \n",
            "19           -6.177386           -6.645034           -6.798252   \n",
            "20           -3.326449           -2.962379           -2.978257   \n",
            "21           -0.999329           -1.030468           -0.945398   \n",
            "22           -6.889293           -6.637251           -6.756453   \n",
            "23           -3.763090           -3.718502           -3.602236   \n",
            "24           -4.106104           -3.616718           -3.657045   \n",
            "25           -0.871901           -0.719842           -0.522287   \n",
            "26           -0.752497           -1.891117           -1.740462   \n",
            "27           -0.195544           -0.039985           -0.119893   \n",
            "28          -13.918474          -12.903371          -11.400370   \n",
            "29          -11.631861           -9.608311           -8.095709   \n",
            "\n",
            "    cloze_8WordContext  cloze_9WordContext  cloze_10WordContext  \n",
            "0                  NaN                 NaN                  NaN  \n",
            "1                  NaN                 NaN                  NaN  \n",
            "2                  NaN                 NaN                  NaN  \n",
            "3                  NaN                 NaN                  NaN  \n",
            "4                  NaN                 NaN                  NaN  \n",
            "5                  NaN                 NaN                  NaN  \n",
            "6                  NaN                 NaN                  NaN  \n",
            "7                  NaN                 NaN                  NaN  \n",
            "8           -10.124781                 NaN                  NaN  \n",
            "9            -7.661354           -7.632751                  NaN  \n",
            "10           -5.923632           -5.348027            -4.988402  \n",
            "11           -4.507034           -4.802739            -2.024833  \n",
            "12           -3.564043           -3.413126            -2.668010  \n",
            "13           -2.212337           -1.099865            -1.125110  \n",
            "14          -13.157067          -12.440165           -13.787453  \n",
            "15           -8.988011           -9.069675            -9.036148  \n",
            "16           -5.734196           -5.890073            -5.679624  \n",
            "17           -0.544204           -0.620501            -0.552791  \n",
            "18           -1.916438           -2.269666            -2.445270  \n",
            "19           -6.545548           -6.631099            -6.543444  \n",
            "20           -2.797942           -2.282179            -2.534775  \n",
            "21           -0.818535           -0.767024            -0.950733  \n",
            "22           -6.610821           -6.321725            -5.944431  \n",
            "23           -3.807438           -3.849402            -3.453249  \n",
            "24           -3.021421           -3.005506            -2.681247  \n",
            "25           -0.382183           -0.729057            -0.413168  \n",
            "26           -1.545825           -1.310650            -1.550995  \n",
            "27           -0.083504           -0.066444            -0.052855  \n",
            "28          -13.156653          -12.356569           -11.929349  \n",
            "29           -7.362024           -8.310596            -7.907667  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}